{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Here's an explanation of the data scraping process based on the code provided:\n",
        "\n",
        "1. **User Search by Location and Followers**: The code begins by defining a `get_users_in_basel` function that queries GitHub's API to find users located in Boston with more than 100 followers. It constructs a search URL using GitHub’s search API, with parameters for location, follower count, and pagination, allowing it to fetch users in batches of 100 per page.\n",
        "\n",
        "2. **Data Retrieval and Pagination**: Using a while loop, the script iterates over pages of results until it reaches the end (when the number of items fetched is less than 100). This ensures it gathers all available users fitting the search criteria. Each user's basic information is saved into a list.\n",
        "\n",
        "3. **Detailed User Data**: For each user found, the `get_user_details` function is called to retrieve additional information from each user's individual profile, including details like name, company, location, and bio. It also uses `clean_company_name` to standardize company names for consistency.\n",
        "\n",
        "4. **Repository Data**: After collecting detailed user profiles, the `get_user_repos` function fetches each user’s repositories. This function retrieves data on repositories, including the number of stars, primary language, and license type.\n",
        "\n",
        "5. **Data Storage in CSV**: The script saves the user profiles and repositories to separate CSV files (`users.csv` and `repositories.csv`) using `save_users_to_csv` and `save_repos_to_csv`, making the data available for further analysis or use."
      ],
      "metadata": {
        "id": "43j7Sg_jqhSW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Eq3vBWa1cK",
        "outputId": "b55cc43e-4f2c-4017-be2b-1af4e572e846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "GITHUB_TOKEN = \"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\"\n",
        "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
        "\n",
        "def get_users_in_basel():\n",
        "    users = []\n",
        "    query = \"location:Boston+followers:>100\"\n",
        "    page = 1\n",
        "    per_page = 100\n",
        "    total_users = 0\n",
        "\n",
        "    while True:\n",
        "        url = f\"https://api.github.com/search/users?q={query}&per_page={per_page}&page={page}\"\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        print(f\"Fetching page {page}...\")\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Error fetching data:\", response.json())\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        users.extend(data['items'])\n",
        "        total_users += len(data['items'])\n",
        "\n",
        "        if len(data['items']) < per_page:\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    detailed_users = []\n",
        "    for user in users:\n",
        "        user_info = get_user_details(user['login'])\n",
        "        detailed_users.append(user_info)\n",
        "\n",
        "    return detailed_users\n",
        "\n",
        "def get_user_details(username):\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    user_data = requests.get(user_url, headers=HEADERS).json()\n",
        "\n",
        "    return {\n",
        "        'login': user_data['login'],\n",
        "        'name': user_data['name'],\n",
        "        'company': clean_company_name(user_data['company']),\n",
        "        'location': user_data['location'],\n",
        "        'email': user_data['email'],\n",
        "        'hireable': user_data['hireable'],\n",
        "        'bio': user_data['bio'],\n",
        "        'public_repos': user_data['public_repos'],\n",
        "        'followers': user_data['followers'],\n",
        "        'following': user_data['following'],\n",
        "        'created_at': user_data['created_at'],\n",
        "    }\n",
        "\n",
        "def clean_company_name(company):\n",
        "    if company:\n",
        "        company = company.strip().upper()\n",
        "        if company.startswith('@'):\n",
        "            company = company[1:]\n",
        "    return company\n",
        "\n",
        "def get_user_repos(username):\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos?per_page=500\"\n",
        "    response = requests.get(repos_url, headers=HEADERS)\n",
        "    repos_data = response.json()\n",
        "\n",
        "    repos = []\n",
        "    for repo in repos_data:\n",
        "        repos.append({\n",
        "            'login': username,\n",
        "            'full_name': repo['full_name'],\n",
        "            'created_at': repo['created_at'],\n",
        "            'stargazers_count': repo['stargazers_count'],\n",
        "            'watchers_count': repo['watchers_count'],\n",
        "            'language': repo['language'],\n",
        "            'has_projects': repo['has_projects'],\n",
        "            'has_wiki': repo['has_wiki'],\n",
        "            'license_name': repo['license']['key'] if repo['license'] else None,\n",
        "        })\n",
        "\n",
        "    return repos\n",
        "\n",
        "def save_users_to_csv(users):\n",
        "    with open('users.csv', mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['login', 'name', 'company', 'location', 'email', 'hireable', 'bio', 'public_repos', 'followers', 'following', 'created_at'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(users)\n",
        "\n",
        "def save_repos_to_csv(repos):\n",
        "    with open('repositories.csv', mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['login', 'full_name', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'has_projects', 'has_wiki', 'license_name'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(repos)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    users = get_users_in_basel()\n",
        "    save_users_to_csv(users)\n",
        "\n",
        "    all_repos = []\n",
        "    for user in users:\n",
        "        repos = get_user_repos(user['login'])\n",
        "        all_repos.extend(repos)\n",
        "\n",
        "    save_repos_to_csv(all_repos)\n",
        "    print(\"Done\")\n"
      ]
    }
  ]
}